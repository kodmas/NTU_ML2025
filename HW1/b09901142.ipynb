{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      },
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWQh-lq8GuwZ",
        "outputId": "26e21947-8f5c-4f60-ae34-da3986f685b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5Tf1rMHBQ-",
        "outputId": "8cad437d-0a1f-45d4-88de-82bf6a3f49da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BS/大五/ML\n"
          ]
        }
      ],
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd \"./drive/MyDrive/BS/大五/ML/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JywoPOO_Oll",
        "outputId": "5d89e4c8-4b0e-437c-cea4-ba52efcb04e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m182.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.4.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.1.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6SizAt_Olm",
        "outputId": "b8fc14f8-800c-437b-be11-f75a5a1534ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyW45N__Olm",
        "outputId": "6303b877-b265-497f-d1e6-f6ba648d69ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "import re\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "def filter_garbled(text: str) -> str:\n",
        "    \"\"\"\n",
        "    This function removes characters that are not in typical ranges for Chinese,\n",
        "    Latin letters, numbers, or common punctuation.\n",
        "    \"\"\"\n",
        "    # The regex below allows:\n",
        "    # - Chinese characters (\\u4e00-\\u9fff)\n",
        "    # - Latin letters and digits (A-Za-z0-9)\n",
        "    # - Whitespace and common punctuation\n",
        "    pattern = re.compile(r'[^\\u4e00-\\u9fffA-Za-z0-9\\s,\\.!?;:\"“”‘’()（）\\-]')\n",
        "    return pattern.sub('', text)\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "def window_contains_keyword(text: str, keyword: str, window_size: int = 200) -> bool:\n",
        "    \"\"\"\n",
        "    Slide a window over the text and check if any window contains the keyword.\n",
        "    The check is case-insensitive.\n",
        "    \"\"\"\n",
        "    keyword_lower = keyword.lower()\n",
        "    text_lower = text.lower()\n",
        "    keyword = keyword.split(\" \")\n",
        "    if len(text) <= window_size:\n",
        "        return keyword_lower in text_lower\n",
        "    # Use overlapping windows (step is half the window size).\n",
        "    step = max(window_size // 2, 1)\n",
        "    for i in range(0, len(text) - window_size + 1, step):\n",
        "        window = text_lower[i:i+window_size]\n",
        "        for keyw in keyword:\n",
        "          # print(\"keyw = \", keyw)\n",
        "          if keyw in window:\n",
        "              # print(\"here\")\n",
        "              return True\n",
        "    return False\n",
        "\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    print(\"keyword = \", keyword)\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh-tw\", unique=True))\n",
        "    # print(\"intermediate results = \", results)\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # print(\"result after get_htmls = \", results)\n",
        "    # Filter out the None values.\n",
        "    html_results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    soups = [BeautifulSoup(html, 'html.parser') for html in html_results]\n",
        "    final_results = []\n",
        "    for soup in soups:\n",
        "        text = ''.join(soup.get_text().split())\n",
        "        # Only proceed if the text encoding is UTF-8.\n",
        "        if detect(text.encode()).get('encoding') != 'utf-8':\n",
        "            continue\n",
        "        # Remove unwanted characters.\n",
        "        cleaned_text = filter_garbled(text)\n",
        "        # Check if the text looks like random gibberish.\n",
        "        # print(\"cleaned_text = \", cleaned_text)\n",
        "        # Ensure that at least one sliding window of text contains the keyword.\n",
        "        # if not window_contains_keyword(cleaned_text, keyword, 5000):\n",
        "        #     continue\n",
        "        final_results.append(cleaned_text[:5000])\n",
        "\n",
        "    return final_results[:n_results]\n",
        "    # results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "\n",
        "    # # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    # results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # # Return the first n results.\n",
        "    # return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmGCARd_Oln",
        "outputId": "4f529a3b-e691-45fe-a77e-fcad45e571fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大熱作如 《1989》（2014年）、_reputation（）和 _Lover （）。她的歌曲經常探討愛情、友誼及自我成長等主題。\n",
            "\n",
            "泰勒絲獲得了許多獎項，包括13座格萊美奖，並且是史上最快達到百萬銷量的女藝人之一。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str,search_results:List=[]) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            if len(search_results) > 0:\n",
        "              messages = [\n",
        "                  {\"role\": \"system\", \"content\": f\"{self.role_description} 必須使用繁體中文回答問題。\"},\n",
        "                  {\n",
        "                      \"role\": \"user\",\n",
        "                      \"content\": f\"任務描述：\\n{self.task_description}\\n\\n搜尋結果：\\n{search_results}\\n\\n使用者問題：\\n{message}\",\n",
        "                  },\n",
        "              ]\n",
        "            # Format the messsages first.\n",
        "            else:\n",
        "              messages = [\n",
        "                  {\"role\": \"system\", \"content\": f\"{self.role_description}\" + \" 必須使用繁體中文回答問題。\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"任務描述：\\n{self.task_description}\\n\\n使用者訊息：\\n{message}\"},\n",
        "              ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"您是一位專業的問題摘要專家，專精於將長問題摘要成簡潔的核心問題。\",\n",
        "    task_description=\"請仔細閱讀以下長問題，並將其摘要成一個更短、更精煉的版本。請確保摘要能表達提問重點。請勿回答問題本身。請僅輸出摘要後的短問題本身。請使用繁體中文。\",\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"您是一位專業的關鍵詞提取專家，專精於從長問題中提取出最能代表其核心提問的關鍵詞。\",\n",
        "    task_description=\"請仔細閱讀以下長問題，並提取出最能代表其核心提問的關鍵詞。請確保這些關鍵詞能概括提問重點。請僅輸出以空格分隔的關鍵詞本身。請使用繁體中文。\",\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"您是一位專業的問答系統，專精於根據提供的搜尋結果和使用者問題，提供準確的答案。\",\n",
        "    task_description=\"請根據以下提供的搜尋結果和使用者問題，回答使用者的問題。請確保答案基於搜尋結果。請僅輸出問題的答案本身。請使用繁體中文。\",\n",
        ")\n",
        "\n",
        "qa_agent_simple = LLMAgent(\n",
        "    role_description=\"您是一位專業的問答系統，專精於根據使用者的問題，提供準確的答案。\",\n",
        "    task_description=\"請根據以下提供的使用者問題，正確回答問題，並僅輸出問題的答案本身。請使用繁體中文。\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    # print(\"original question = \", question)\n",
        "    if(len(question) > 10):\n",
        "      keywords = keyword_extraction_agent.inference(question,[])\n",
        "      abstract = question_extraction_agent.inference(question,[])\n",
        "      search_results = await search(keywords,3)\n",
        "      # print(\"keywords = \", keywords)\n",
        "      # print(\"abstract = \", abstract)\n",
        "      # print(\"serach_results = \", search_results[:][:10])\n",
        "      ans = qa_agent.inference(abstract,search_results)\n",
        "    else:\n",
        "      ans = qa_agent_simple.inference(question, [])\n",
        "\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plUDRTi_B39S",
        "outputId": "fe3ee7a0-cded-46dd-8f48-9b11c736d268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original question =  在《遊戲王》卡牌遊戲中，以「真紅眼黑龍」與「黑魔導」作為融合素材的融合怪獸是什麼？\n",
            "\n",
            "keyword =  真紅眼 黑魔導 鐵甲龍\n",
            "keywords =  真紅眼 黑魔導 鐵甲龍\n",
            "abstract =  《遊戲王》中以「真紅眼黑龍」與 「 黑魔導 」作為融合素材的哪隻怪獸？\n",
            "47 真紅眼黑龍\n",
            "original question =  豐田萌繪在《BanG Dream!》企劃中，擔任哪個角色的聲優？\n",
            "\n",
            "keyword =  豐田萌繪  BanG Dream! 角色 聲優\n",
            "keywords =  豐田萌繪  BanG Dream! 角色 聲優\n",
            "abstract =  豐田萌繪在《BanG Dream!》中聲演哪個角色？\n",
            "48 松原花音\n",
            "original question =  Rugby Union 中，9 號球員的正式名稱為何？\n",
            "\n",
            "keyword =  Scrum半back\n",
            "keywords =  Scrum半back\n",
            "abstract =  九號球員的正式名稱是掃劍手。\n",
            "49 Scrum-half。\n",
            "original question =  曾被視為太陽系中的行星，最終被降格成矮行星的星球為何？\n",
            "\n",
            "keyword =  矮行星\n",
            "keywords =  矮行星\n",
            "abstract =  什麼是曾被視為太陽系中的行星，最終卻遭到降格成矮天體的那顆恥辱之球？\n",
            "50 冥王星\n",
            "original question =  以往政府對動物保護的觀念僅停留在寵物，因此動保法又被調侃為可愛動物保護法，近年來政策逐漸重視野生動物的保護。臺灣最早成立的野生動物救傷單位位於哪個行政區內？\n",
            "\n",
            "keyword =  野生動物  臺灣政府政策重視保護\n",
            "keywords =  野生動物  臺灣政府政策重視保護\n",
            "abstract =  臺灣最早成立的野生動物救傷單位位於哪個行政區內？\n",
            "51 台北市立動物園\n",
            "original question =  位於南投縣集集鎮的特生中心是親子育樂的好去處，館內以臺灣本土生態及生物為主軸，規劃高、中、低海拔生態系、特有動物、特有植物、環境-生物-人、自然保育、植物的奧秘及動物的奇觀等主題展區。特生中心在2023年改名，目前該單位的名字為？\n",
            "\n",
            "keyword =  特生中心  南投縣集集中    親子育樂   本土生物\n",
            "keywords =  特生中心  南投縣集集中    親子育樂   本土生物\n",
            "abstract =  特生中心目前的名字是什麼？\n",
            "52 特有生物研究保育中心\n",
            "original question =  Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data論文中提出的模型是甚麼名字？\n",
            "\n",
            "keyword =  Instruction-Following Speech Language Model\n",
            "keywords =  Instruction-Following Speech Language Model\n",
            "abstract =  提問重點：模型名稱\n",
            "53 DeSTA2\n",
            "original question =  請問太陽系中體積最大的行星是哪一顆？\n",
            "\n",
            "keyword =  太陽系 行星 體積\n",
            "keywords =  太陽系 行星 體積\n",
            "abstract =  太陽系中體積最大的行星是哪一顆？\n",
            "54 太陽系中體積最大的行星是木衛三。\n",
            "original question =  在語言分類學上，臺灣目前法定的十六個原住民族語言皆屬於南島語系，然其中一族的語言與其他語言在分類學上一般不被視為同一群，請問是哪一族的語言與其他語言親緣關係最遙遠？\n",
            "\n",
            "keyword =  南島語系  原住民族 語言分類學\n",
            "keywords =  南島語系  原住民族 語言分類學\n",
            "abstract =  哪一族的原住民族語言與其他南島系臺灣十六個法定認定的原始民間在分類學上最不相近？\n",
            "55 泰雅族\n",
            "original question =  相傳在台灣大學的某一堂程式設計課程從來不會進行分組，如果有人問為甚麼沒有分組，老師總會說「我上課都不分組的。分組的結果通常都是一個阿宅在寫code；一個未來可以當PM的在台上吹牛；一個廢柴跑去買便當」。請問講出這句話的老師是誰？\n",
            "\n",
            "keyword =  程式設計課 老師 分組 阿宅 PM 廢柴\n",
            "keywords =  程式設計課 老師 分組 阿宅 PM 廢柴\n",
            "abstract =  老師是誰？\n",
            "56 劉邦鋒\n",
            "original question =  臺灣原住民族的語言非常多元，有各式各樣的打招呼用語，比方說布農族的「uninang mihumisang」、阿美族的「nga'ay ho」。「embiyax namu kana」 是哪一臺灣原住民族的打招呼用語？\n",
            "\n",
            "keyword =  布農族 原住民族 打招呼用語\n",
            "keywords =  布農族 原住民族 打招呼用語\n",
            "abstract =  哪一種臺灣原住民族的打招呼用語是「embiyax namu kana」？\n",
            "57 太魯閣族\n",
            "original question =  鄒族與布農族生活區域大量重疊，最開始鄒族因為驍勇善戰，因此擁有大量土地，後來因為外族人帶來的瘟疫，使得鄒族的族群勢力快速下滑，並與布農族人混居。請問「鄒與布農，永久美麗」這句話與哪個鄒族、布農族混居的部落息息相關？\n",
            "\n",
            "keyword =  混居 布農族 鄒족 部落 瘟疫\n",
            "keywords =  混居 布農族 鄒족 部落 瘟疫\n",
            "abstract =  請問「鄒與布農，永久美麗」這句話是哪個部落的口頭禪？\n",
            "58 這句話是布農族的口頭禪。\n",
            "original question =  動畫「雖然是公會的櫃檯小姐，但因為不想加班所以打算獨自討伐迷宮頭目」中女主角隱藏的冒險者身份是甚麼？\n",
            "\n",
            "keyword =  關鍵詞：動畫  女主角 隱藏 身份 冒險者\n",
            "keywords =  關鍵詞：動畫  女主角 隱藏 身份 冒險者\n",
            "abstract =  女主角隱藏的冒險者身份是甚麼？\n",
            "59 蓓兒丹娣（Belldandy）是女主角，隱藏的冒險者身份是一位神屬法術士。\n",
            "original question =  在卑南族的傳說中，有一對姊弟（Tuku 及 Sihasihau），這對姊弟後來各自創建了一個部落，其中姊姊 Tuku 後來創建了哪一個部落？\n",
            "\n",
            "keyword =  卑南族  Tuku姊弟部落創建\n",
            "keywords =  卑南族  Tuku姊弟部落創建\n",
            "abstract =  在卑南族的傳說中，姊弟 Tuku 及 Sihasihau 分別創建了哪兩個部落？\n",
            "60 射馬干部落的創社人杜姑（卑南語Tuku）為卡大地布社會族祖西哈 西浩 （ 卞 南 說 Sihasihau ） 的 姐姐。\n",
            "original question =  2005 播出的電視劇《終極一班》中，有一個高中生戰力排行榜，稱為「KO榜」，該榜榜首為？\n",
            "\n",
            "keyword =  高中生戰力排行榜 KO 排名 終極一班\n",
            "keywords =  高中生戰力排行榜 KO 排名 終極一班\n",
            "abstract =  《終極一班》中的「KO榜」第一名是誰？\n",
            "61 KO榜第一名是汪大東。\n",
            "original question =  Linux kernel 曾使用的 process scheduler --- completely fair scheduler (CFS) 採用了何種資料結構來儲存排程相關資訊？\n",
            "\n",
            "keyword =  process scheduler CFS 資料結構\n",
            "keywords =  process scheduler CFS 資料結構\n",
            "abstract =  Linux kernel 中的 CFS 使用紅黑樹（Red-Black Tree）來儲存排程相關資訊。\n",
            "62 紅黑樹（Red-Black Tree）在 Linux kernel 中的 CFS 使用是基於時序順向排序。\n",
            "original question =  請問第二次世界大戰中，盟軍在歐洲發動的最大規模登陸作戰諾曼第登陸（Normandy Landings），其作戰代號為何？\n",
            "\n",
            "keyword =  Operation Overlord\n",
            "keywords =  Operation Overlord\n",
            "abstract =  盟軍在諾曼第登陸的作戰代號是「奧運會」（Operation Overlord）。\n",
            "63 盟軍在諾曼第登陸的作戰代號是「奧運會」（Operation Overlord）。\n",
            "original question =  《Cytus II》遊戲中「Body Talk」是哪位角色的歌曲？\n",
            "\n",
            "keyword =  角色  Cytus II\n",
            "keywords =  角色  Cytus II\n",
            "abstract =  《Cytus II》遊戲中「Body Talk」是哪位角色歌曲？\n",
            "64 Body Talk 是 Cherry 的歌曲。\n",
            "original question =  李琳山教授 1974 年畢業於國立臺灣大學電機工程學系，並且在 1975 年及 1977 年由美國史丹佛大學取得電機工程碩士及博士學位。他在國立臺灣大學所開設的信號與系統課程，在期末考前後會有一次演講，該演講又被稱為？\n",
            "\n",
            "keyword =  李琳山教授  國立臺灣大學電機工程學系 信號與系統課程 演講\n",
            "keywords =  李琳山教授  國立臺灣大學電機工程學系 信號與系統課程 演講\n",
            "abstract =  李琳山教授的演講又被稱為什麼？\n",
            "65 信號與系統\n",
            "original question =  唉，我朋友總是一直說他很窮，買不起輝達最新的 5090 顯卡。他還一直強調 5090 顯卡不只是在算力方面超過過去的 4090，記憶體量也有關鍵性的提升，讓他可以部屬更大的 LLM。那 RTX 5090 的 VRAM 到底是多大？\n",
            "\n",
            "keyword =  顯卡 5090 RTX 記憶體量 LLM\n",
            "keywords =  顯卡 5090 RTX 記憶體量 LLM\n",
            "abstract =  你朋友的顯卡需求是否與實際性能相符？\n",
            "66 根據搜尋結果顯示，NVIDIA GeForce RTX 5090 的實際性能遠超出朋友的需求。該卡擁有21,760個CUDA核心、680个TensorCore和176 个RT Core，以及32GB GDDR7記憶體，這使得它在遊戲計算密集型工作負載以及AI加速方面具有無與倫比的情況下，能夠提供高達8倍的效率提升。\n",
            "original question =  棒球一直是風靡全台灣的運動之一，台灣棒球歷史也相當的悠久，自日治時期的台灣開始便有棒球運動的紀錄。而中華職棒作為台灣目前唯一的職業棒球聯盟，更是台灣棒球的重要象徵之一。台灣的棒球也享譽國際，是各大棒球國際賽的常客之一。請問在2024年所舉辦的「世界棒球12強賽」中，冠軍為哪一隊？\n",
            "\n",
            "keyword =  世界棒球12強賽 2024冠軍\n",
            "keywords =  世界棒球12強賽 2024冠軍\n",
            "abstract =  2024年世界棒球12強賽冠軍隊伍是哪一支？\n",
            "67 中華隊\n",
            "original question =  中國四大奇書是哪幾本？\n",
            "\n",
            "keyword =  中國四大奇書  奇蹟   文學\n",
            "keywords =  中國四大奇書  奇蹟   文學\n",
            "abstract =  中國四大奇書是哪幾本？\n",
            "68 中國四大奇書是指水滸傳、西遊記、三國演義和金瓶梅。\n",
            "original question =  中國時辰中的子時，如果用24小時制表示，是幾點到幾點？\n",
            "\n",
            "keyword =  子時 23:00 - 次日01點\n",
            "keywords =  子時 23:00 - 次日01點\n",
            "abstract =  子時的時間範圍是？\n",
            "69 子時的時間範圍是從晚上11點（即23:00）至隔日凌晨1分鐘之間。\n",
            "original question =  一般學生總是有許多課堂作業需要完成，通常學生們會順著作業的死線將時序拉近，因為遲交作業可能讓作業不算分或是作業分數打折。而電腦也不例外，電腦有著排程演算法來決定接下來要執行什麼動作。請問在作業系統中，避免要錯過時限來完成作業的排程演算法稱為什麼？\n",
            "\n",
            "keyword =  時間表演算法 Deadline Scheduling\n",
            "keywords =  時間表演算法 Deadline Scheduling\n",
            "abstract =  在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「即期式」或是\"Real-time Scheduling\"?\n",
            "70 是的，避免要錯過時限來完成任務的一種排程演算法稱為「即期式」或\"Real-time Scheduling\"?\n",
            "original question =  在2010年代初期由日本輕小說改編的知名動畫系列《刀劍神域》中，存在一組以英文字母「C」開頭搭配四位數「8763」構成的招式代號，此招式因施展時會出現十六道連續斬擊軌跡與金色光效而廣為觀眾所知。請具體回答：該代號「C8763」在原作中明確對應哪位角色持有的劍技？\n",
            "\n",
            "keyword =  刀劍神域 C8763\n",
            "keywords =  刀劍神域 C8763\n",
            "abstract =  《刀劍神域》中哪位角色持有「C8763」招式代號？\n",
            "71 《刀劍神域》中，主角桐人持有「C8763」招式代號，即星爆氣流斬。\n",
            "original question =  曾經風靡一時的影集《斯卡羅》描述的是斯卡羅族的故事，劇中之地名「柴城」位於現今的哪個行政區劃？\n",
            "\n",
            "keyword =  斯卡羅族 柴城 行政區劃\n",
            "keywords =  斯卡羅族 柴城 行政區劃\n",
            "abstract =  《斯卡羅》劇中地名「柴城」位於現今的哪個行政區劃？\n",
            "72 屏東縣\n",
            "original question =  Google Colab的訂閱制中，若要使用A100高級GPU，需要訂閱「Colab Pro」還是「Colab Pro+」？\n",
            "\n",
            "keyword =  Google Colab A100 高級 GPU 訂閱\n",
            "keywords =  Google Colab A100 高級 GPU 訂閱\n",
            "abstract =  Google Colab的訂閱制中，A100高級GPU使用需求。\n",
            "73 Google Colab的訂閱制中，A100高級GPU使用需求為每小時13.08個單元。\n",
            "original question =  台灣大學中由李宏毅老師開設的機器學習，是屬於哪個學院的課程？\n",
            "\n",
            "keyword =  機器學習 李宏毅 老師 台大\n",
            "keywords =  機器學習 李宏毅 老師 台大\n",
            "abstract =  李宏毅老師開設的機器學習課程屬於哪個學院？\n",
            "74 李宏毅老師開設的機器學習課程屬於國立台灣大學電力工程學院（即現今稱為「電子與資訊工科」）。\n",
            "original question =  就讀國立臺灣大學資工系的雪江同學，正在為 113 學年度第 2 學期選課的事情煩惱著，身為一位大三學生，他正在考慮兩個修課的策略。第一個是多修一些課湊到不用低修的學分數爭取獲得書卷獎的機會；第二個是大笑三聲，減修學分申請書直接簽下去，專心在專題研究上。如果雪江同學選擇第一個策略，它至少要修多少學分才可以不用簽減修學分申請書？\n",
            "\n",
            "keyword =  書卷獎 低修學分數\n",
            "keywords =  書卷獎 低修學分數\n",
            "abstract =  雪江同學選擇第一個策略，至少要修多少分數才不用簽減免申請書？\n",
            "75 至少20分數\n",
            "original question =  知名 AI VTuber「Neuro-sama」最初的 Live2D 模型是使用 VTube Studio 的哪個角色？\n",
            "\n",
            "keyword =  Neuro-sama VTube Studio Live2D 模型\n",
            "keywords =  Neuro-sama VTube Studio Live2D 模型\n",
            "abstract =  Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 哪個角色？\n",
            "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio AshArmsZero。\n",
            "original question =  「Re：從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是誰？\n",
            "\n",
            "keyword =  愛蜜莉雅 劫持 人物\n",
            "keywords =  愛蜜莉雅 劫持 人物\n",
            "abstract =  從零開始的異世界生活 第三季中，劫持愛蜜莉雅並想取其為妻的人是誰？\n",
            "77 從零開始的異世界生活第三季中，劫持愛蜜莉雅並想取其為妻的人是加菲爾。\n",
            "original question =  《海綿寶寶》的主角海綿寶寶在第五季《失蹤記》中，在哪個城市擊敗刺破泡泡紅眼幫？\n",
            "\n",
            "keyword =  海綿寶宝  第五季   失蹤記\n",
            "keywords =  海綿寶宝  第五季   失蹤記\n",
            "abstract =  海綿寶宝在第五季《失蹤記》中，在哪個城市擊敗刺破泡沫紅眼幇？\n",
            "78 答案：没有相关信息\n",
            "original question =  玉米是單子葉還是雙子葉植物？\n",
            "\n",
            "keyword =  玉米 雙子葉 植物\n",
            "keywords =  玉米 雙子葉 植物\n",
            "abstract =  玉米是雙子葉植物。\n",
            "79 不是的，玉米是一種單子葉植物。\n",
            "original question =  中華民國陸軍，隸屬於國防部陸軍司令部，總兵力約為13.2萬，兵力為三軍之最。請問中華民國陸軍軍歌前六字為何？\n",
            "\n",
            "keyword =  陸軍 軍歌 中華民國\n",
            "keywords =  陸軍 軍歌 中華民國\n",
            "abstract =  中華民國陸軍的歌曲前六字是什麼？\n",
            "80 風雲起山河動\n",
            "original question =  台大的理工科系大多規定一些基礎自然科學科目，如普通物理和普通化學，作為必修學分，這似乎已成為「普通」跟「理所當然」的規定。但同時也有一些科系，由於科系的專業幾乎並無這些自然科學知識的用武之地，因此便將這些科目的必修學分要求較為放寬。請問台大電資學院哪個系規定物理、化學以及生物科目可以只擇一修習即可？\n",
            "\n",
            "keyword =  電資學院  資訊工程系\n",
            "keywords =  電資學院  資訊工程系\n",
            "abstract =  台大電資學院哪個系的物理、化學以及生物科目可以只擇一修習即可？\n",
            "81 根據搜尋結果，台大電資學院有以下系所提供相關科目：  * 電子工程學研究所   + 可能包含物理、化工等課程，但具體內容需查詢該院的教師與研究方向。    然而，並無明確指出可以只擇一修習即可。\n",
            "original question =  在月球的火山地形中，廣大而平坦的玄武岩平原地形被稱為月海、月灣或月湖。憂傷湖（Lacus Doloris）、死湖（Lacus Mortis）、忘湖（Lacus Oblivionis）、恐怖湖（Lacus Timoris）以及愛灣（Sinus Amoris），以上五個地形何者位於不面對地球的月球背面？\n",
            "\n",
            "keyword =  月球背面  月海\n",
            "keywords =  月球背面  月海\n",
            "abstract =  哪些月海/灣不面對地球？\n",
            "82 月球背面的海/灣包括：東方海道，莫斯科湾（Moscow Bay），智慧的危險地帶和史密夫湖。\n",
            "original question =  請問由貝多芬所創作的《C♯小調第14號鋼琴奏鳴曲》，其較為人知的別稱是什麼？\n",
            "\n",
            "keyword =  貝多芬  鋼琴奏鳴曲\n",
            "keywords =  貝多芬  鋼琴奏鳴曲\n",
            "abstract =  《C♯小調第14號鋼琴奏鳴曲》又稱為什麼？\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》又稱為「月光」。\n",
            "original question =  阿米斯音樂節是在國際上頗負盛名的音樂展演活動，請問阿米斯音樂節是由哪位歌手所舉辦？\n",
            "\n",
            "keyword =  阿米斯音樂節 歌手\n",
            "keywords =  阿米斯音樂節 歌手\n",
            "abstract =  阿米斯音樂節的舉辦者是誰？\n",
            "84 舒米恩都蘭部落族人\n",
            "original question =  「Poppy Playtime - Chapter 4」遊戲中出現的黏土人叫甚麼名字？\n",
            "\n",
            "keyword =  黏土人 名字\n",
            "keywords =  黏土人 名字\n",
            "abstract =  遊戲「Poppy Playtime - Chapter 4」中的黏土人名字是甚麼？\n",
            "85 根據搜尋結果，黏土人的名字是由於其材質感和造型而得名。\n",
            "original question =  賓茂部落 Djumulj 的現址是 1951 年遷村後的新聚落，Djumulj 在排灣族語裡是「經常豐收，糧食堆積如山之地」的意思。賓茂村是一塊飛地。在地理位置上，賓茂被太麻里鄉所包圍，但賓茂其實屬於何一行政區劃？\n",
            "\n",
            "keyword =  賓茂部落  Djumulj   排灣族語    遷村後的新聚 落 飛地 太麻里鄉 行政區劃\n",
            "keywords =  賓茂部落  Djumulj   排灣族語    遷村後的新聚 落 飛地 太麻里鄉 行政區劃\n",
            "abstract =  賓茂村的行政區劃屬於哪一鄉鎮？\n",
            "86 賓茂村的行政區劃屬於金峰鄉。\n",
            "original question =  義大利文藝復興時期著名雕塑家米開朗基羅創作的《大衛》雕像，最初是在何處雕刻並展現其雕塑藝術成就？\n",
            "\n",
            "keyword =  米開朗基羅  大衛雕像\n",
            "keywords =  米開朗基羅  大衛雕像\n",
            "abstract =  米開朗基羅《大衛》雕像的創作地點是哪裡？\n",
            "87 佛羅倫斯。\n",
            "original question =  中華民國國軍的軍階中，最高級的將領為特級上將，肩、領章上有五顆星星。除了蔣中正之外，另一位曾短暫晉升特級上將的將領是誰？\n",
            "\n",
            "keyword =  蔣中正  特級上將\n",
            "keywords =  蔣中正  特級上將\n",
            "abstract =  除了蔣中正之外，曾短暫晉升特級上將的另一位軍官是誰？\n",
            "88 李烈鈞\n",
            "original question =  線上遊戲「英雄聯盟」2012年第二賽季世界大賽的總冠軍是哪一個戰隊？\n",
            "\n",
            "keyword =  英雄聯盟 2012 年第二賽季世界大赛\n",
            "keywords =  英雄聯盟 2012 年第二賽季世界大赛\n",
            "abstract =  2012年第二賽季世界大赛的總冠軍是哪支戰隊？\n",
            "89 台北暗殺星（TaipeiAssassins）\n",
            "original question =  在日本麻將中，非莊家一開始的手牌有幾張？\n",
            "\n",
            "keyword =  非莊家 14 張\n",
            "keywords =  非莊家 14 張\n",
            "abstract =  日本麻將非莊家一開始的手牌有幾張？\n",
            "90 根據搜尋結果，日本麻將非莊家一開始的手牌有13張。\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"b09901142\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "        # break\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d43Oq8GzFKkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}